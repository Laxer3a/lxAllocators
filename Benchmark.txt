Preliminary Benchmark:
	5000 malloc per loop, in task system.
	Tested on a Bloomfield i7 920 @ 2700 Mhz FIXED, 4 Cores available, no Hyperthreading activated.
	
Stack Allocator : Allocation in million per seconds.
=====================================================

			MT Enable			ST Only
	1 Core	73.9M/Sec			144.4M/Sec
	4 Cores	21.4M/Sec (Shared)	575.5M/Sec (/Thread)
			292 M/Sec (/Thread)
			
Average Cycle count per alloc per core.
=====================================================
	
			MT Enable			ST Only
	1 Core	 36 Cycles			18.6 Cycles
	4 Cores	504 Cycles (Shared)	18.7 Cycles
			 36 Cycles (/Thread)
	
	Single thread optimized could is roughly twice faster than MT lockless lock even without sharing data and object across core.
	
	Performance degredation during usage in multicore usage with share allocator is that allocator pointer are on the same cache line.
	But it is not something that can be optimized by allocating a specific cache line per thread as we need to access atomic values.

	
Standard malloc/free : Allocation in million per seconds.
	malloc & free support multithreading.
=====================================================
			MT Enable			ST Only
	1 Core	1.2  M/Sec			N/A (Same)
	4 Cores	0.95 M/Sec (Shared)	N/A (Same)
			0.95 M/Sec (/Thread)

Average Cycle count per alloc per core.
=====================================================
	
			MT Enable			ST Only
	1 Core	 2250 Cycles			N/A
	4 Cores	 2840 Cycles (Shared)	N/A
			 2840 Cycles (/Thread)

	Well, one can conclude why we should NOT rely on a system allocator except allocation big chunks for smaller allocators.
	May the benchmark by allocating pieces of aligned memory of 128 byte for 20 byte block isnt the best thing we can do.
	Seems there is a few % gain when using non aligned version of the function. (1.3 millions but could my measurement)

Pool Allocator : ALLOCATION + FREE in million per seconds.
=====================================================

			MT Enable			ST Only
	1 Core	---------			 90M/Sec			 30 Cycles per alloc + free.
	4 Cores	--------- (Shared)	340M/Sec (/Thread)	 31 Cycles per alloc + free per core.
			--------- (/Thread)

	MT Enable with ((power of 2-)1) item count
	1 Core	 35.5 M/Sec				 76 Cycles per alloc + free
	4 Cores	 11.7 M/Sec(Shared)		230 Cycles per alloc + free
			138.5 M/Sec(/Thread)	 78 Cycles per alloc + free per core.

	MT Enable with any generic size
	1 Core	 22.3 M/Sec				122 Cycles per alloc + free
	4 Cores	  9.6 M/Sec(Shared)		282 Cycles per alloc + free
			 74.0 M/Sec(/Thread)	145 Cycles per alloc + free per core.
			 
	It is obvious that specific optimized part in a single thread provide at least twice the performance of the optimized
	equivalent supporting multithreading.
	But, as scalability does not improve but rather degrade when sharing the allocator, it would be
	recommended if possible to use a pool for each allocator.
	
	Note that of course the benchmark is putting a lot of pressure on the allocator, mostly accessing the same cache line
	all the time doing nothing else, so this is an extreme worst case.
	Still, even in this case, we are 10x more efficient than a standard malloc.
	
	It is important to notice that using a specific size of item count gives you a 30% improvement in performance,
	as some logic is not necessary anymore.
	
	Pool allocator, unlike stack allocator has the ability to recycle and free elements, so working a few pools
	based on different size slot can really give a edge in term of performance.
	At 30 cycles including the cost of disallocation, it gets close in term of competitivity to the stack allocator.

